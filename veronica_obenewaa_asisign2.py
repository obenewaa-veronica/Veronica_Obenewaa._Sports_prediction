# -*- coding: utf-8 -*-
"""Veronica_Obenewaa_Asisign2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xn7x--Iw11CqZIGEAfOqKlNKrDyxqj18
"""

# Importing relevant libraries

import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
warnings.simplefilter(action='ignore', category=Warning) # Suppress warnings
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score

from google.colab import drive
drive.mount('/content/drive')

"""### Data Preparation & Feature Extraction"""

# To display all the columns of the dataset:
pd.set_option('display.max_columns', None)
data_male_players= pd.read_csv('/content/drive/My Drive/Veronica/Assignment 2/New/male_players.csv')
data_players_22= pd.read_csv('/content/drive/My Drive/Veronica/Assignment 2/New/players_22.csv')

male_players = pd.DataFrame(data_male_players)
players_22 = pd.DataFrame(data_players_22)

male_players.head()

players_22.head()

male_players.describe()

# gives a description of the dataset player_21
players_22.describe()

# getting information about the male_players_legacy dataset

male_players.info()

# getting information about the players_22 dataset
players_22.info()

male_players.shape

players_22.shape

male_players.columns

# viewing the columns of the dataset player_22
players_22.columns

players_22.hist(bins=50, figsize=(20,15))
plt.show()

male_players.hist(bins=50, figsize=(20,15))
plt.show()

"""Rename the overall column to overall rating"""

# Rename the column: overall to overall_rating

male_players.rename(columns={'overall':'overall_rating', },inplace=True)
players_22.rename(columns={'overall':'overall_rating', },inplace=True)

"""Dropping some redundant columns




"""

# Removing columns with more than 30% missing values and also selecting columns without any NaN values
male_players = male_players.loc[:, (male_players.isnull().mean() <= 0.3) | (male_players.isnull().sum() == 0)]
players_22 = players_22.loc[:, (players_22.isnull().mean() <= 0.3) | (players_22.isnull().sum() == 0)]

# Drop the following columns in male_players:

male_players.drop(['mentality_composure'], axis=1, inplace=True)
male_players.drop(['player_id'], axis=1, inplace=True)
male_players.drop(['nationality_id'], axis=1, inplace=True)
male_players.drop(['club_team_id'], axis=1, inplace=True)
male_players.drop(['league_id'], axis=1, inplace=True)

# Drop the following columns in players_22:
players_22.drop(['sofifa_id'], axis=1, inplace=True)

male_players.shape

players_22.shape

"""Checking for null values"""

# for col in male_players:
male_players.isnull().sum()

# for col in players_22:
players_22.isnull().sum()

# Print the columns names containing null values or missing values in the form of a list

# For male_players
missing_values_for_male_players = male_players.isnull().sum()
columns_with_missing_values_for_male_players = missing_values_for_male_players[missing_values_for_male_players > 0].index.tolist()

# For players_22
missing_values_for_players_22 = players_22.isnull().sum()
columns_with_missing_values_for_players_22 = missing_values_for_players_22[missing_values_for_players_22 > 0].index.tolist()

print(columns_with_missing_values_for_male_players,"\n")
print(columns_with_missing_values_for_players_22)

"""Extracting Non-Categorical Features"""

# Keep the following categorical features for male_players
preferred_foot_for_male_players = male_players['preferred_foot']
work_rate_for_male_players = male_players['work_rate']

# Keep the following categorical features for players_22
preferred_foot_for_players_22 = players_22['preferred_foot']
work_rate_for_players_22 = players_22['work_rate']

# Extract numerical features

num_cols_for_male_players = male_players.select_dtypes(include=['number']).columns
num_cols_for_players_22 = players_22.select_dtypes(include=['number']).columns
print(num_cols_for_male_players, "\n")
print(num_cols_for_players_22)

male_players = male_players[num_cols_for_male_players].head()
male_players

players_22 = players_22[num_cols_for_players_22].head()
players_22

# Print the columns names containing null values or missing values in the form of a list

# For male_players
missing_values_for_male_players = male_players.isnull().sum()
columns_with_missing_values_for_male_players = missing_values_for_male_players[missing_values_for_male_players > 0].index.tolist()

print(columns_with_missing_values_for_male_players,"\n")

# Print the columns names containing null values or missing values in the form of a list

# For male_players
missing_values_for_players_22 = players_22.isnull().sum()
columns_with_missing_values_for_players_22 = missing_values_for_players_22[missing_values_for_players_22 > 0].index.tolist()

print(columns_with_missing_values_for_players_22)

# Fill NaN values with the mean of the column


male_players = male_players.fillna(male_players.mean())

# Again, Print the columns names containing null values or missing values in the form of a list

# For male_players
missing_values_for_male_players = male_players.isnull().sum()
columns_with_missing_values_for_male_players = missing_values_for_male_players[missing_values_for_male_players > 0].index.tolist()

print(columns_with_missing_values_for_male_players,"\n")

male_players.head()

players_22.head()

# Convert categorical features into numeric for male_players
preferred_foot_for_male_players = pd.get_dummies(preferred_foot_for_male_players, prefix='preferred_foot')
work_rate_for_male_players  = pd.get_dummies(work_rate_for_male_players , prefix='work_rate')


# Convert categorical features into numeric for players_22
preferred_foot_for_players_22 = pd.get_dummies(preferred_foot_for_players_22, prefix='preferred_foot')
work_rate_for_players_22 = pd.get_dummies(work_rate_for_players_22, prefix='work_rate')

preferred_foot_for_male_players

work_rate_for_male_players

preferred_foot_for_players_22

work_rate_for_players_22

male_players

players_22

# Again, Print the columns names containing null values or missing values in the form of a list

# For male_players
missing_values_for_male_players = male_players.isnull().sum()
columns_with_missing_values_for_male_players = missing_values_for_male_players[missing_values_for_male_players > 0].index.tolist()

print(columns_with_missing_values_for_male_players,"\n")

# Join both transformed numeric and non-numeric columns to form fully numeric columns
male_players = pd.concat([male_players, preferred_foot_for_male_players, work_rate_for_male_players,], axis=1)

players_22 = pd.concat([players_22, preferred_foot_for_players_22, work_rate_for_players_22], axis=1)

male_players.shape

players_22.shape

## At this point male_players only has numeric features in the dataset. Lets impute the missing values if any.

# for male_players
imp = SimpleImputer()

# Fit and transform the data
try:
    imputed_data_for_male_players = imp.fit_transform(male_players)
    male_players = pd.DataFrame(imputed_data_for_male_players, columns=male_players.columns)
    print(male_players)
except ValueError as e:
    print("Error during imputation:", e)

## At this point players_21 and players_22 only have numeric features in the dataset. Lets impute the missing values.

# for players_22
imp = SimpleImputer(strategy='most_frequent')
imputed_data_22 = imp.fit_transform(players_22)
players_22 = pd.DataFrame(imputed_data_22, columns=players_22.columns)

# Print the columns names containing null values or missing values in the form of a list

# For male_players
missing_values_for_male_players = male_players.isnull().sum()
columns_with_missing_values_for_male_players = missing_values_for_male_players[missing_values_for_male_players > 0].index.tolist()

print(columns_with_missing_values_for_male_players,"\n")

# Print the columns names containing null values or missing values in the form of a list

# For players_22
missing_values_for_players_22 = players_22.isnull().sum()
columns_with_missing_values_for_players_22 = missing_values_for_players_22[missing_values_for_players_22 > 0].index.tolist()

print(columns_with_missing_values_for_players_22)

players_22

male_players

"""Feature Selection"""

# Calculate the correlation between each feature and the target variable
correlation_matrix = male_players.corr()
target_correlation = correlation_matrix['overall_rating'].drop('overall_rating')

# Select the top-k features with the highest absolute correlation
k = 20
top_k_features = target_correlation[target_correlation > 0.5].abs().nlargest(k).index

# Collect the column names of top_k_features in a list and store it in the 'selected_features' variable
selected_features = top_k_features.tolist()

# Display the selected features and their correlation with the target variable
print(f"\nTop {k} features with maximum correlation with the target variable:\n")
print(male_players[top_k_features])

'''
#The correlation matrix produces the some results for players_22

# Calculate the correlation between each feature and the target variable
correlation_matrix = players_22.corr()
target_correlation = correlation_matrix['overall_rating'].drop('overall_rating')

# Select the top-k features with the highest absolute correlation
k = 20
top_k_features = target_correlation[target_correlation > 0.5].abs().nlargest(k).index

# Collect the column names of top_k_features in a list and store it in the 'selected_features' variable
selected_features = top_k_features.tolist()

# Display the selected features and their correlation with the target variable
print(f"\nTop {k} features with maximum correlation with the target variable:\n")
print(players_22[top_k_features])

print(selected_features)

'''

# List of features that would be trained
selected_features

# Select the the independent (X) and dependent (Y) variables
X = male_players[selected_features]
y = male_players['overall_rating']

X.head()

y.head()

X.shape

y.shape

"""**Feature Scaling and Training**"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

## View the dimension of the training data and testing data
x_train.shape, y_test.shape, y_train.shape, x_test.shape

"""**Ensemble Models**"""

# Model 1: Train RandomForest, XGBoost and Gradient Boost Regressors models with cv and grid search:
models = {
    'RandomForest': RandomForestRegressor(),
    'XGBoost': XGBRegressor(),
    'GradientBoost': GradientBoostingRegressor()
}

params = {
    'RandomForest': {'n_estimators': [4, 5], 'max_depth': [None, 5]},
    'XGBoost': {'n_estimators': [5, 6], 'learning_rate': [0.01, 0.1]},
    'GradientBoost': {'n_estimators': [4, 6], 'learning_rate': [0.01, 0.1]}
}

for name, model in models.items():
    gs = GridSearchCV(model, params[name], cv=5)
    gs.fit(x_train, y_train)
    # Make predictions using the best model from GridSearchCV
    y_pred = gs.predict(x_test)

    # Evaluate the model
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\nBest parameters for {name}: {gs.best_params_}")
    print(f"Validation score for {name}: {gs.score(x_test, y_test)}")
    print(f"mean_absolute_error for {name}: {mae}")
    print(f"mean_squared_error for {name}: {mse}",)
    print(f"r2_score for {name}: {r2}\n")

rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    random_state=42
)

# Train the model
rf_model.fit(x_train, y_train)
y_pred = rf_model.predict(x_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("mean_absolute_error:",mae)
print("mean_squared_error:",mse)
print("r2_score:",r2)

# Model 2: VotingClassifier

decision_tree = DecisionTreeClassifier(random_state=42, criterion='entropy')
knn = KNeighborsClassifier(n_neighbors=8)
svm = SVC(probability=True, random_state=42)

voting_classifier = VotingClassifier(estimators=[
    ('decision_tree', decision_tree),
    ('knn', knn),
    ('svm', svm)
], voting='soft')

for model in (decision_tree, knn, svm,voting_classifier):
  model.fit(x_train,y_train)
  y_pred=model.predict(x_test)
  print(model.__class__.__name__,accuracy_score(y_pred,y_test))

# Model 3: RandomForestClassifier

rfc=RandomForestClassifier(n_estimators=20, max_depth=3, criterion='entropy')

# Perform cross-validation
cv_scores = cross_val_score(rfc, x_train, y_train, cv=5)
print(f"\nCross-validation scores: {cv_scores}")
print(f"\nMean cross-validation score: {cv_scores.mean()}")

# Fit the model
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test)
accuracy_score(y_pred,y_test)
print('\nAccuracy of the model:',accuracy_score(y_pred,y_test))

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("\nmean_absolute_error of the model:",mae)
print("\nmean_squared_error of the model:",mse)
print("\nr2_score of the model:",r2)

# Fine-tune the model (RandomForestClassifier) with GridSearchCV
n_estimators_range = list(range(1, 31))

# Create a parameter grid: map the parameter names to the values that should be searched
param_grid = dict(n_estimators=n_estimators_range)

grid = GridSearchCV(RandomForestClassifier(max_depth=3, criterion='entropy'), param_grid, cv=10, scoring='accuracy')
grid.fit(x_train, y_train)

rfc=RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=3, criterion='entropy')
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test)
accuracy_score(y_pred,y_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Examine the best model
print("\ngrid.best_score:",grid.best_score_)
print("\ngrid.best_params:",grid.best_params_)
print("\ngrid.best_estimator:",grid.best_estimator_)
print("\nAccuracy of the best model:", accuracy_score(y_pred,y_test))
print("\nmean_absolute_error of the best model:",mae)
print("\nmean_squared_error of the best model:",mse)
print("\nr2_score of the best model:",r2)

# correlation between variables in selected_features and the target variable; overall_rating
for name, score in zip(x_train.columns, rfc.feature_importances_):
  print(name, score)

"""**Testing the models using players_22**"""

x_test_22 = players_22[selected_features]
y_test_22 = players_22['overall_rating']

scaler = StandardScaler()
x_test_22_scaled = scaler.fit_transform(x_test_22)
x_test_22 = pd.DataFrame(x_test_22_scaled, columns=x_test_22.columns)

# Model 1: Testing players_22 on RandomForest, XGBoost, Gradient Boost Regressors
models = {
    'RandomForest': RandomForestRegressor(),
    'XGBoost': XGBRegressor(),
    'GradientBoost': GradientBoostingRegressor()
}

params = {
    'RandomForest': {'n_estimators': [4, 5], 'max_depth': [None, 5]},
    'XGBoost': {'n_estimators': [5, 6], 'learning_rate': [0.01, 0.1]},
    'GradientBoost': {'n_estimators': [4, 6], 'learning_rate': [0.01, 0.1]}
}

for name, m in models.items():
    gs = GridSearchCV(m, params[name], cv=5)
    gs.fit(x_train, y_train)
    # Make predictions using the best model from GridSearchCV
    y_pred = gs.predict(x_test_22)

    # Evaluate the model
    mae = mean_absolute_error(y_test_22, y_pred)
    mse = mean_squared_error(y_test_22, y_pred)
    r2 = r2_score(y_test_22, y_pred)

    #print('\nAccuracy: {}'.format(nb_model.score(x_test, y_test)))
    print(f"\nBest parameters for {name}: {gs.best_params_}")
    print(f"Validation score for {name}: {gs.score(x_test_22, y_test_22)}")
    #print()
    print(f"mean_absolute_error for {name}: {mae}")
    print(f"mean_squared_error for {name}: {mse}",)
    print(f"r2_score for {name}: {r2}\n")

# Model 2: Testing players_22 on VotingClassifier

decision_tree = DecisionTreeClassifier(random_state=42, criterion='entropy')
knn = KNeighborsClassifier(n_neighbors=8)
svm = SVC(probability=True, random_state=42)

voting_classifier = VotingClassifier(estimators=[
    ('decision_tree', decision_tree),
    ('knn', knn),
    ('svm', svm)
], voting='soft')

for model in (decision_tree, knn, svm,voting_classifier):
  model.fit(x_train,y_train)
  y_pred=model.predict(x_test_22)
  print(model.__class__.__name__,accuracy_score(y_pred,y_test_22))

# Model 3: Testing players_22 on RandomForestClassifier

rfc=RandomForestClassifier(n_estimators=20, max_depth=3, criterion='entropy')

# Perform cross-validation
cv_scores = cross_val_score(rfc, x_train, y_train, cv=5)
print(f"\nCross-validation scores: {cv_scores}")
print(f"\nMean cross-validation score: {cv_scores.mean()}")

# Fit the model
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test_22)
accuracy_score(y_pred,y_test_22)
print('\nAccuracy of the model:',accuracy_score(y_pred,y_test_22))

# Evaluate the model
mae = mean_absolute_error(y_test_22, y_pred)
mse = mean_squared_error(y_test_22, y_pred)
r2 = r2_score(y_test_22, y_pred)
print("\nmean_absolute_error of the model:",mae)
print("\nmean_squared_error of the model:",mse)
print("\nr2_score of the model:",r2)

# Fine-tune the model (RandomForestClassifier) with GridSearchCV
n_estimators_range = list(range(1, 31))

# Create a parameter grid: map the parameter names to the values that should be searched
param_grid = dict(n_estimators=n_estimators_range)

grid = GridSearchCV(RandomForestClassifier(max_depth=3, criterion='entropy'), param_grid, cv=10, scoring='accuracy')
grid.fit(x_train, y_train)

rfc=RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=3, criterion='entropy')
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test_22)
accuracy_score(y_pred,y_test_22)

# Evaluate the model
mae = mean_absolute_error(y_test_22, y_pred)
mse = mean_squared_error(y_test_22, y_pred)
r2 = r2_score(y_test_22, y_pred)

# Examine the best model
print("\ngrid.best_score:",grid.best_score_)
print("\ngrid.best_params:",grid.best_params_)
print("\ngrid.best_estimator:",grid.best_estimator_)
print("\nAccuracy of the best model:", accuracy_score(y_pred,y_test_22))
print("\nmean_absolute_error of the best model:",mae)
print("\nmean_squared_error of the best model:",mse)
print("\nr2_score of the best model:",r2)

# correlation between variables in selected_features and the target variable; overall_rating
for name, score in zip(x_train.columns, rfc.feature_importances_):
  print(name, score)

"""**Saving RandomForestRegressor model using pickle**"""

filename = 'player_rating_predictor.pkl'
pickle.dump(rf_model, open(filename, 'wb'))

loaded_model = pickle.load(open(filename, 'rb'))

y_pred = loaded_model.predict(x_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("mean_absolute_error:",mae)
print("mean_squared_error:",mse)
print("r2_score:",r2)

if isinstance(loaded_model, RandomForestRegressor):
    print("The model is a RandomForestRegressor.")
else:
    print("The model is not a RandomForestRegressor.")